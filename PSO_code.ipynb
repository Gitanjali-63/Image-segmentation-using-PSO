{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSO-code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoPpKgXO7XGloIyHKBKWMQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gitanjali-63/Image-segmentation-using-PSO/blob/main/PSO_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gmqWdf_Qa_I"
      },
      "outputs": [],
      "source": [
        "from natsort import natsorted \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost\n",
        "import cv2\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import os\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "id": "HX_kzMVaU8HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA ANALYSIS\n",
        "data=pd.read_csv('Brain Tumor.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "xi9pnlH5VwXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCALING\n",
        "scalable=['Mean', 'Variance', 'Standard Deviation', 'Entropy',\n",
        "       'Skewness', 'Kurtosis', 'Contrast', 'Energy', 'ASM', 'Homogeneity',\n",
        "       'Dissimilarity', 'Correlation', 'Coarseness']\n",
        "\n",
        "\n",
        "data[scalable]=StandardScaler().fit_transform(data[scalable])\n",
        "data"
      ],
      "metadata": {
        "id": "_IW-j5zmV0cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA CORRELATIONS\n",
        "sns.swarmplot(x=y,y= data['Homogeneity'])\n",
        "plt.title(\"Distribution of image Homogenity, by Class\")"
      ],
      "metadata": {
        "id": "btWL66aCV4Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class1=data['Class']== 1\n",
        "class0=data['Class']== 0\n",
        "_data=data.copy()\n",
        "_data=data.drop('Image',axis=1,inplace=False)\n",
        "sns.distplot(a= _data[class1]['Energy'], label=\"Tumor\")\n",
        "sns.distplot(a = _data[class0]['Energy'], label=\"No tumor\" )\n",
        "\n",
        "plt.title(\"Distribution of image Energy, by Class\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "4XR9TRqSV7ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(a= _data[class1]['Entropy'], label=\"Tumor\")\n",
        "sns.distplot(a = _data[class0]['Entropy'], label=\"No tumor\" )\n",
        "plt.title(\"Distribution of image Entropy, by Class\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "YAAT63GwV_xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "1_k4HaXqWDae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q-HXSjrcWGxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wk9m4o4yWNwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()  \n",
        "folder='/content/Image3760.jpg'\n",
        "imgs=natsorted(folder)\n",
        "\n",
        "img=cv2.imread(folder,cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "im = plt.imshow(img,  interpolation='none', aspect='auto',cmap ='gray', vmin=0, vmax=255)   \n",
        "plt.title('No Tumor')"
      ],
      "metadata": {
        "id": "mL4WCp9rWORy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()  \n",
        "folder='/content/Image3.jpg'\n",
        "imgs=natsorted(folder)\n",
        "\n",
        "img=cv2.imread(folder,cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "im = plt.imshow(img,  interpolation='none', aspect='auto',cmap ='gray', vmin=0, vmax=255)   \n",
        "plt.title('Tumor')\n",
        "\n"
      ],
      "metadata": {
        "id": "SoBrLKPiWRik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=data.Class\n",
        "y"
      ],
      "metadata": {
        "id": "hSHn8MSqWo38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Most impactful features\n",
        "from collections import OrderedDict\n",
        "\n",
        "model = xgboost.XGBRegressor(colsample_bytree=0.4,\n",
        "                 gamma=0,                 \n",
        "                 learning_rate=0.07,\n",
        "                 max_depth=3,\n",
        "                 min_child_weight=1.5,\n",
        "                 n_estimators=10000,                                                                    \n",
        "                 reg_alpha=0.75,\n",
        "                 reg_lambda=0.45,\n",
        "                 subsample=0.6,\n",
        "                 seed=42) \n",
        "model.fit(data.drop(['Image','Class'],axis=1,inplace=False),y)\n",
        "OrderedDict(sorted(model.get_booster().get_fscore().items(),key=lambda t: t[1], reverse=True))"
      ],
      "metadata": {
        "id": "ORFZ44yTWvW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BRAIN TUMOR PREDICTION\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "ZikTKw2HWwG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yes_data = []\n",
        "\n",
        "no_data = []\n",
        "\n",
        "paths = []\n",
        "\n",
        "labels = []\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Brain Tumor'):\n",
        "    \n",
        "    for filename in filenames:\n",
        "        \n",
        "        if '.jpg' in filename:\n",
        "            \n",
        "            paths.append(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "aP9ySujQWzXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()\n",
        "\n",
        "encoder.fit([[0], [1]])\n",
        "\n",
        "for path in paths:\n",
        "    \n",
        "    image = Image.open(path)\n",
        "    \n",
        "    image = image.resize((128,128))\n",
        "    \n",
        "    image= np.array(image)\n",
        "    \n",
        "    if image.shape == (128,128,3):\n",
        "        \n",
        "        yes_data.append(np.array(image))\n",
        "        \n",
        "        labels.append(encoder.transform([[0]]).toarray())\n",
        "        \n",
        "labels[0]"
      ],
      "metadata": {
        "id": "KLEyi2N9W2wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "more_paths = []\n",
        "\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Brain Tumor/no'):\n",
        "    \n",
        "    for filename in filenames:\n",
        "        \n",
        "        if '.jpg' in filename:\n",
        "            \n",
        "            more_paths.append(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "fjMdeRHAW_E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for path in more_paths:\n",
        "    \n",
        "    image = Image.open(path)\n",
        "    \n",
        "    image = image.resize((128,128))\n",
        "    \n",
        "    image = np.array(image)\n",
        "    \n",
        "    if image.shape == (128,128,3):\n",
        "        \n",
        "        yes_data.append(np.array(image))\n",
        "        \n",
        "        labels.append(encoder.transform([[1]]).toarray())"
      ],
      "metadata": {
        "id": "J0DGOcexW_wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yes_data = np.array(yes_data)\n",
        "\n",
        "yes_data.shape"
      ],
      "metadata": {
        "id": "Ek-dHqjpXGkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(labels)\n",
        "\n",
        "labels = labels.reshape(3762,2)\n",
        "\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "zoMYhFZVXHHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(linewidth = 200)\n",
        "\n",
        "plt.imshow(yes_data[0])\n",
        "\n",
        "print(yes_data[0])\n",
        "\n",
        "print(labels[0])"
      ],
      "metadata": {
        "id": "cFz2dOT5XKFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training + Testing data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(yes_data, labels, random_state = 3, shuffle = True)"
      ],
      "metadata": {
        "id": "3vSmFiY1XMNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizing Data\n",
        "train_data = train_data /255.0\n",
        "test_data = test_data/255.0"
      ],
      "metadata": {
        "id": "Bjgxj8fKXRI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "\n",
        "input = Input(shape = (128,128,3))\n",
        "\n",
        "x = Conv2D(16, (3,3), activation = 'relu')(input)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "\n",
        "x = Conv2D(32, (3,3), activation = 'relu')(x)\n",
        "\n",
        "x = MaxPooling2D(2,2)(x)\n",
        "\n",
        "\n",
        "\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Dense(124, activation = 'relu')(x)\n",
        "\n",
        "x = Dropout(0.27)(x)\n",
        "\n",
        "x = Dense(124, activation = 'relu')(x)\n",
        "\n",
        "output = Dense(2, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(inputs = input, outputs = output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TDg5abl-XT8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import rmsprop_v2"
      ],
      "metadata": {
        "id": "-YZlMs8pXXxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy',  metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "G_kFsm9wXbDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a Learning Rate Scheduler\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8*10**(epoch / 20))"
      ],
      "metadata": {
        "id": "ZEwdMX3CXeS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting results\n",
        "# accuracy\n",
        "\n",
        "plt.figure(figsize = (12,7))\n",
        "\n",
        "plt.plot(history.history['accuracy'], label = 'train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'validation accuracy')\n",
        "plt.title(\"train accuracy vs validation accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HvA0g7UjXhPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "\n",
        "plt.figure(figsize = (12,7))\n",
        "\n",
        "plt.plot(history.history['loss'], label = 'train loss')\n",
        "plt.plot(history.history['val_loss'], label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.title(\"train vs validation loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KxnvDlElXlgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions\n",
        "def tumor(number):\n",
        "    \n",
        "    if number == 0:\n",
        "        \n",
        "        return \"Not a tumor\"\n",
        "    \n",
        "    elif nimber ==1:\n",
        "        \n",
        "        return \"a tumor\""
      ],
      "metadata": {
        "id": "00bySG08XpHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example image\n",
        "\n",
        "img = Image.open(\"/content/Image3760.jpg\")\n",
        "\n",
        "x = np.array(img.resize((128,128)))\n",
        "\n",
        "x = x.reshape(1, 128, 128, 3)\n",
        "\n",
        "result = model.predict([x])\n",
        "\n",
        "classification = np.where(result == np.amax(result))[1][0]\n",
        "\n",
        "print(str(result[0][classification]*100) + '% Confidence This Is ' + tumor(classification))\n",
        "\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "xGrXL8n4XvM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img2 = Image.open(r\"/content/Image70.jpg\")\n",
        "\n",
        "y = np.array(img2.resize((128,128)))\n",
        "\n",
        "y = y.reshape(1, 128, 128, 3)\n",
        "\n",
        "result2 = model.predict([y])\n",
        "\n",
        "ind = 0\n",
        "classification2 = 0\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for result in result2:\n",
        "    result = result.tolist()\n",
        "    print(result)\n",
        "    \n",
        "    if result[0] > result[1]:\n",
        "        classification = 1\n",
        "        ind = 0\n",
        "    else:\n",
        "        classification = 0\n",
        "        ind = 1\n",
        "\n",
        "#classification2 = np.where(result2 == np.amax(result2))[1][0]\n",
        "\n",
        "print(str(result2[0][ind]*100) + '% Confidence This Is ' + tumor(classification2))\n",
        "\n",
        "plt.imshow(img2)"
      ],
      "metadata": {
        "id": "aikLsGjoXxv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img3 = Image.open(r\"/content/Image67.jpg\")\n",
        "\n",
        "y = np.array(img3.resize((128,128)))\n",
        "\n",
        "y = y.reshape(1, 128, 128, 3)\n",
        "\n",
        "result2 = model.predict([y])\n",
        "\n",
        "ind = 0\n",
        "classification2 = 0\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for result in result2:\n",
        "    result = result.tolist()\n",
        "    print(result)\n",
        "    \n",
        "    if result[0] > result[1]:\n",
        "        classification = 1\n",
        "        ind = 0\n",
        "    else:\n",
        "        classification = 0\n",
        "        ind = 1\n",
        "\n",
        "#classification2 = np.where(result2 == np.amax(result2))[1][0]\n",
        "\n",
        "print(str(result2[0][ind]*100) + '% Confidence This Is ' + tumor(classification2))\n",
        "\n",
        "plt.imshow(img3)"
      ],
      "metadata": {
        "id": "qb7zE7kaX6_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn import svm\n",
        "from mpl_toolkits.mplot3d import axes3d, Axes3D"
      ],
      "metadata": {
        "id": "JRmTvKeFYH4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('Brain Tumor (1).csv')"
      ],
      "metadata": {
        "id": "Ty_EJiXJQdlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.plot()"
      ],
      "metadata": {
        "id": "saAxSzISQhaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "tCanqN-YQlNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del data['Image']"
      ],
      "metadata": {
        "id": "JZf6bWz8QnU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizing the data\n",
        "\n",
        "data_n = data.copy()\n",
        "data_n = (data - data.min())/(data.max() - data.min())\n",
        "\n",
        "print(data_n)"
      ],
      "metadata": {
        "id": "unBOfc6DQoAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimensions=12\n",
        "data_cn=pd.concat([data_n.shift(i) for i in range(0+dimensions+1)],axis=1)\n",
        "print(data_cn)"
      ],
      "metadata": {
        "id": "m68SaY5aQq1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into train, validation and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "x=data_cn.iloc[12:,1:]\n",
        "y=data_cn.iloc[12:,0]\n",
        "\n",
        "x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.4,shuffle=False)\n",
        "\n",
        "x_val,x_test,y_val,y_test=train_test_split(x_val,y_val,test_size=0.5,shuffle=False)\n",
        "\n",
        "print(len(y_val))\n",
        "print(len(y_test))\n",
        "print(len(y_train))"
      ],
      "metadata": {
        "id": "Y5JIml-NQuWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PSO \n",
        "from sklearn.model_selection import train_test_split\n",
        "def pso(n_particles, iterations, dimensions, inertia):\n",
        "\n",
        "    # Range of SVR's hyperparameters (Particles' search space)\n",
        "    # C, Epsilon and Gamma\n",
        "    max_c = 1e4\n",
        "    min_c = 1e-3\n",
        "    max_e = 1e-1\n",
        "    min_e = 1e-8\n",
        "    max_g = 1e3\n",
        "    min_g = 1e-3\n",
        "    \n",
        "    # Initializing particles' positions randomly, inside\n",
        "    # the search space\n",
        "    x = np.random.rand(n_particles, 1)*(max_c - min_c) + min_c\n",
        "    y = np.random.rand(n_particles, 1)*(max_e - min_e) + min_e\n",
        "    z = np.random.rand(n_particles, 1)*(max_g - min_g) + min_g\n",
        "\n",
        "    c = np.concatenate((x,y,z), axis=1)\n",
        "\n",
        "    # Initializing particles' parameters\n",
        "    v = np.zeros((n_particles, dimensions))\n",
        "    c1 = 2\n",
        "    c2 = 2\n",
        "    p_best = np.zeros((n_particles, dimensions))\n",
        "    p_best_val = np.zeros(n_particles) + sys.maxsize  \n",
        "    g_best = np.zeros(dimensions)\n",
        "    g_best_val = sys.maxsize\n",
        "\n",
        "    best_iter = np.zeros(iterations)\n",
        "\n",
        "\n",
        "    # Initializing regression variables\n",
        "    p_best_RGS = np.empty((n_particles), dtype = object);\n",
        "    g_best_RGS = sys.maxsize\n",
        "\n",
        "    \n",
        "\n",
        "    # Displaying tridimensional search space\n",
        "    plot(c)\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn import metrics\n",
        "    \n",
        "    for i in range(iterations):\n",
        "\n",
        "        for j in range(n_particles):\n",
        "          # Starting Regression\n",
        "          rgs = svm.SVR(C = c[j][0], epsilon = c[j][1], gamma = c[j][2])\n",
        "          x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.4,shuffle=False)\n",
        "          x_val,x_test,y_val,y_test=train_test_split(x_val,y_val,test_size=0.5,shuffle=False)\n",
        "          # Fitting the curve\n",
        "          rgs.fit(x_train,y_train)\n",
        "          y_predict = rgs.predict(x_val)\n",
        "\n",
        "          # Using Mean Squared Error to verify prediction accuracy\n",
        "          mse = mean_squared_error(y_val, y_predict) \n",
        "\n",
        "          # If mse value for that search point, for that particle,\n",
        "          # is less than its personal best point,\n",
        "          # replace personal best\n",
        "          if(mse < p_best_val[j]):   # mse < p_best_val[j]\n",
        "          # The value below represents the current least Mean Squared Error\n",
        "              p_best_val[j] = mse\n",
        "              \n",
        "              p_best_RGS[j] = rgs\n",
        "                           \n",
        "\n",
        "              # The value below represents the current search coordinates for\n",
        "              # the particle's current least Mean Squared Error found\n",
        "              p_best[j] = c[j].copy()\n",
        "              \n",
        "          # Using auxiliar variable to get the index of the\n",
        "          # particle that found the configuration with the \n",
        "          # minimum MSE value\n",
        "          aux = np.argmin(p_best_val)        \n",
        "        \n",
        "          if(p_best_val[aux] < g_best_val):\n",
        "              # Assigning Particle's current best MSE to the Group's best    \n",
        "              g_best_val = p_best_val[aux]\n",
        "\n",
        "              # Assigning Particle's current best configuration to the Group's best\n",
        "              g_best = p_best[aux].copy()\n",
        "\n",
        "              # Group best regressor:\n",
        "              # the combination of C, Epsilon and Gamma\n",
        "              # that computes the best fitting curve\n",
        "              g_best_RGS = p_best_RGS[aux]\n",
        "\n",
        "        \n",
        "          rand1 = np.random.random()\n",
        "          rand2 = np.random.random()\n",
        "\n",
        "          # The variable below influences directly the particle's velocity.\n",
        "          # It can either make it smaller or bigger. \n",
        "          w = inertia\n",
        "\n",
        "          # The equation below represents Particle's velocity, which is\n",
        "          # the rate of change in its position\n",
        "          v[j] = w*v[j] + c1*(p_best[j] - c[j])*rand1 + c2*(g_best - c[j])*rand2\n",
        "\n",
        "          # Change in the Particle's position \n",
        "          c[j] = c[j] + v[j]\n",
        "\n",
        "          # Below is a series of conditions that stop the particles from\n",
        "          # leaving the search space\n",
        "          if(c[j][2] < min_g):\n",
        "            c[j][2] = min_g\n",
        "          if(c[j][2] > max_g):\n",
        "            c[j][2] = max_g\n",
        "          if(c[j][1] < min_e):\n",
        "            c[j][1] = min_e\n",
        "          if(c[j][1] > max_e):\n",
        "            c[j][1] = max_e\n",
        "          if(c[j][0] < min_c):\n",
        "            c[j][0] = min_c\n",
        "          if(c[j][0] > max_c):\n",
        "            c[j][0] = max_c\n",
        "            \n",
        "     \n",
        "        # The variable below represents the least Mean Squared Error\n",
        "        # of the current iteration\n",
        "        best_iter[i] = g_best_val\n",
        "                \n",
        "        print('Best value iteration # %d = %f\\n'%(i, g_best_val))\n",
        "\n",
        "    # Coordinates found after all the iterations\n",
        "    print('Group Best configuration found: ')\n",
        "    print(g_best)\n",
        "    print('\\n')\n",
        "    print('Best Regressor:\\n')\n",
        "    print(g_best_RGS)\n",
        "    print('\\n')\n",
        "    # Displaying the MSE value variation throughout the iterations\n",
        "    t = range(iterations)\n",
        "    plt.plot(t, best_iter, label='Fitness Value')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Displaying Particles' final configuration\n",
        "    plot(c)\n",
        "\n",
        "    # Making the prediction with the best configuration of C, Epsilon and\n",
        "    # Gamma found by the particles\n",
        "    x_val,x_test,y_val,y_test=train_test_split(x_val,y_val,test_size=0.5,shuffle=False)\n",
        "\n",
        "    predict_test = g_best_RGS.predict(x_test) \n",
        "    # Displaying actual values and predicted values for\n",
        "    # Group's best configuration found overall\n",
        "    print(color.BOLD + 'Predictions with the Population Best Value found:\\n' + color.END)\n",
        "    evaluate(predict_test)  "
      ],
      "metadata": {
        "id": "9R73xN6YQxY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "metadata": {
        "id": "cXcjyjBURBMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that displays tridimensional plot\n",
        "def plot(some_list):\n",
        " \n",
        "  ax = Axes3D(plt.figure())\n",
        "  ax.scatter3D(some_list[:,0], some_list[:,1], some_list[:,2], color = 'r')\n",
        "  ax.set_xlabel('$C$', fontsize = 20)\n",
        "  ax.set_ylabel('$\\epsilon$', fontsize = 25)\n",
        "  ax.zaxis.set_rotate_label(False) \n",
        "  ax.set_zlabel('$\\gamma$', fontsize=30, rotation = 0)\n",
        "  ax.zaxis._axinfo['label']['space_factor'] = 1.0\n",
        "  plt.show()\n",
        "\n",
        "  print('\\n')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "J0baXcyFRFAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(predictions):\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    import statistics as st\n",
        "\n",
        "    predict_test = predictions\n",
        "\n",
        "    # To un-normalize the data:\n",
        "    # Multiply the values by\n",
        "    # data.to_numpy().max()\n",
        "\n",
        "    plt.plot(range(len(y_test)), y_test, label='Real')\n",
        "    plt.plot(range(len(predict_test)), predict_test, label='Predicted')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    mse = mean_squared_error(y_test, predict_test)\n",
        "    print('\\n')\n",
        "    print('\\n')\n",
        "    print('Mean Squared Error for the Test Set:\\t %f' %mse)\n",
        "    print('\\n')\n",
        "    print('\\n')\n",
        "    print('Predictions Average:\\t %f' %((predict_test.sum()/len(predict_test))))\n",
        "    print('\\n')\n",
        "    print('\\n')\n",
        "    print('Predictions Median:\\t %f' %(st.median(predict_test)))\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "FKMS6ChBRID0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameter of PSO\n",
        "pso(50, 100, 3, 1)"
      ],
      "metadata": {
        "id": "wgKOcWhGRVcU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}